---
title: Create a new cluster
weight: 50
last_reviewed_on: 2019-12-03
review_in: 3 months
---

# Create a new cluster

## Pre-requisites

Before you begin, there are a few pre-requisites:

- Your GPG key must be added to the [infrastructure repo] so that you are able to run `git-crypt unlock` (the script will run this for you, but you must be *able* to do it)

- You have [AWS CLI] profiles `moj-cp` and `moj-dsd` with suitable credentials

- You have [docker] installed

## Environment Variables

See the file `example.env.create-cluster` in the [infrastructure repo]. This shows examples of the environment variables which must be set in order to run the `create-cluster.rb` script to create a new cluster.

You can get the auth0 values from the `terraform-provider-auth0` application on [auth0](https://manage.auth0.com/#/applications).

## Run the build script, via the tools image

The cloud platform [tools image] has all the software required to run the build script and create a cluster.

Start from the root directory of a working copy of the [infrastructure repo].

With your environment variables set, launch a bash shell on the tools image:

```bash
make tools-shell
```

Execute the script, supplying the desired name of your new cluster. e.g.:

```bash
./create-cluster.rb --name david-test1
```

NB: Your cluster name must be **no more than 12 characters**. Any longer, and some of the computed strings which include the cluster name will exceed their maximum allowed values. The error messages you get if this happens are unhelpful. In order to prevent this, the build script will fail immediately if you supply a name which is too long.

See our [cluster naming policy] for information on how to choose a suitable name for your cluster.

By default, the script will create a 'small' cluster. This means the master and worker EC2 instances will be less powerful machine types than in our production cluster. To create a 'medium' or 'production' cluster, run the script like this:

```bash
./create-cluster.rb --name david-test1 --size medium # or --size production
```

See the script source code for the precise machine types used for the different sizes. The machine types used for 'production' clusters should always be the same machine types as we use in production.

The script takes around 30 minutes to execute. At the end, you should see output like this:

```
Run options: exclude {:cluster=>"live-1"}
..........................

Finished in 4 minutes 53.4 seconds (files took 4.54 seconds to load)
26 examples, 0 failures

2019-10-07 09:07:49 kubectl cluster-info
Kubernetes master is running at https://api.david-test1.cloud-platform.service.justice.gov.uk
KubeDNS is running at https://api.david-test1.cloud-platform.service.justice.gov.uk/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

The section before `kubectl cluster-info` is the cluster integration tests, which run at the end of the build
process. The number of tests will increase, so the output will vary from what is shown here.

Each dot in the line represents a passing test. If any tests fail, you will see more output explaining
the nature of the failure.

Any test failures could indicate a problem with your test cluster, which will need to be resolved before you
can confidently use it for anything.

## Alerts

The build script replaces the webhook values in the source code with dummy values, to prevent
alerts from test clusters from showing up in the team slack channels.

If you want to reverse this, you need to revert the changes to these files, and then do a `terraform apply`:

* high priority alerts: `terraform/cloud-platform-components/terraform.tfvars` (look for `pagerduty_config`)
* lower priority alerts: `terraform/cloud-platform-components/terraform.tfvars` (look for `alertmanager_slack_receivers`)

### Apply the changes

Use terraform to apply the changes to the prometheus operator in your test cluster.

#### Ensure your terraform workspace and kubernetes context are pointing at your test cluster

```bash
cd terraform/cloud-platform-components
terraform workspace list
```

If you are not targeting your test cluster workspace, run

```bash
terraform workspace select [your cluster]
```

Check your kubernetes context by running

```bash
kubectl cluster-info
```

The output should be

```bash
Kubernetes master is running at https://api.XXXXXXX.cloud-platform.service.justice.gov.uk
KubeDNS is running at https://api.XXXXXXX.cloud-platform.service.justice.gov.uk/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

...where `XXXXXXX` is the name of your test cluster.

If your context is pointing at live-1, switch to your test cluster by running

```bash
kops export kubecfg XXXXXXX.cloud-platform.service.justice.gov.uk
```

When you are confident that both your terraform workspace and kubernetes context are set to your test cluster, apply the change to set the pagerduty config value:

```bash
terraform apply -target=helm_release.prometheus_operator
```

Answer `yes` if the planned changes look correct.

## Authenticating

If you want to authenticate to your new cluster in the same way as a regular user would (i.e. logging
in via auth0, rather than using `kops export kubecfg...`), the auth0 login URL is:

```
https://login.apps.[cluster name].cloud-platform.service.justice.gov.uk
```

e.g. for a cluster called "david-test1", the URL to authenticate and download a `kubeconfg` file is:

```
https://login.apps.david-test1.cloud-platform.service.justice.gov.uk
```

[infrastructure repo]: https://github.com/ministryofjustice/cloud-platform-infrastructure
[cluster naming policy]: https://github.com/ministryofjustice/cloud-platform/blob/master/architecture-decision-record/009-Naming-convention-for-clusters.md
[AWS CLI]: https://aws.amazon.com/cli/
[docker]: https://www.docker.com/
[PagerDuty]: https://www.pagerduty.com/
[tools image]: https://github.com/ministryofjustice/cloud-platform-tools-image
