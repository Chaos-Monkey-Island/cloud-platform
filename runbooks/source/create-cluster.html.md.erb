---
title: Create a new cluster
weight: 50
---

# Create a new cluster

## Pre-requisites

Before you begin, there are a few pre-requisites:

- Your GPG key must be added to the [infrastructure repo] so that you are able to run `git-crypt unlock` (the script will run this for you, but you must be *able* to do it)

- You must ensure your local [helm] version is => `2.11` (check by running `helm version`)

- You must have the terraform auth0 provider installed. It isn't listed in the official Terraform repository so follow [the instructions][auth0 provider instructions] to install it.

- You have [kops] installed

- You have the [AWS CLI] tool installed, and profiles `moj-cp` and `moj-dsd` with suitable credentials

- You have [ruby] or [docker] installed

## Environment Variables

See the file `example.env.create-cluster` in the [infrastructure repo]. This shows examples of the environment variables which must be set in order to run the `create-cluster.rb` script to create a new cluster.

You can get the auth0 values from the `terraform-provider-auth0` application on [auth0](https://manage.auth0.com/#/applications).

## Run the build script

> WARNING: Be aware that this script uses the kubernetes context defined on **your machine**, when applying the `cloud-platform-components` terraform files. So, if you have started this script and then used a different terminal to do some other work, it is very easy to forget about the build script and switch your kubernetes context to point to the live cluster. **This can break parts of the production environment.**

Start from the root directory of a working copy of the [infrastructure repo].

Execute the script, supplying the desired name of your new cluster. e.g.:

#### If you have ruby installed

```bash
./create-cluster.rb david-test1
```

#### If you don't have ruby installed

```bash
docker run --rm -v "$PWD":/usr/src/myapp \
  -w /usr/src/myapp ruby:2.6-alpine \
  ruby create-cluster.rb david-test1
```

NB: Your cluster name must be **no more than 12 characters**. Any longer, and some of the computed strings which include the cluster name will exceed their maximum allowed values. The error messages you get if this happens are unhelpful. In order to prevent this, the build script will fail immediately if you supply a name which is too long.

See our [cluster naming policy] for information on how to choose a suitable name for your cluster.

The script takes around 30 minutes to execute. At the end, you should see output like this:

```
Kubernetes master is running at https://api.david-test1.cloud-platform.service.justice.gov.uk
KubeDNS is running at https://api.david-test1.cloud-platform.service.justice.gov.uk/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

## Disable Alerts

By default, your test cluster will send alerts to the team slack channels. This can cause confusion if the work that you're doing on the test cluster involves something which, in the live cluster, would be a critical component.

### High Priority Alerts

The simplest way to disable high-priority alerts is to prevent them from reaching [PagerDuty], by removing the token for our PagerDuty account.

#### Edit the file `terraform/cloud-platform-components/terraform.tfvars`

Find the line `pagerduty_config = "--------- SOME VALUE ----------"`, and replace the value with a dummy value (not empty string), then save the file.


### Lower priority alerts

Low-priority alerts are sent directly to the `#lower-priority-alarms` slack channel, so the procedure for disabling them is slightly different.

#### Edit the file `terraform/cloud-platform-components/terraform.tfvars`

Look for the `alertmanager_slack_receivers` section, and find the entry whose `channel` value is `#lower-priority-alarms`.

Change the value of `webhook` and replace the string after the last `/` in the URL with a dummy value, then save the file.

### Apply the changes

Finally, we will use terraform to apply the changes to the prometheus operator in your test cluster.

#### Ensure your terraform workspace and kubernetes context are pointing at your test cluster

```bash
cd terraform/cloud-platform-components
terraform workspace list
```

If you are not targeting your test cluster workspace, run

```bash
terraform workspace select [your cluster]
```

Check your kubernetes context by running

```bash
kubectl cluster-info
```

The output should be

```bash
Kubernetes master is running at https://api.XXXXXXX.cloud-platform.service.justice.gov.uk
KubeDNS is running at https://api.XXXXXXX.cloud-platform.service.justice.gov.uk/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

...where `XXXXXXX` is the name of your test cluster.

If your context is pointing at live-1, switch to your test cluster by running

```bash
kops export kubecfg XXXXXXX.cloud-platform.service.justice.gov.uk
```

When you are confident that both your terraform workspace and kubernetes context are set to your test cluster, apply the change to set the pagerduty config value:

```bash
terraform apply -target=helm_release.prometheus_operator
```

Answer `yes` if the planned changes look correct.

This should leave all the alerting infrastructure of your test cluster intact, but prevent any high-priority alerts from reaching PagerDuty (and being routed from there to Slack, or anywhere else).

[infrastructure repo]: https://github.com/ministryofjustice/cloud-platform-infrastructure
[auth0 provider instructions]: https://github.com/yieldr/terraform-provider-auth0#using-the-provider
[cluster naming policy]: https://github.com/ministryofjustice/cloud-platform/blob/master/architecture-decision-record/009-Naming-convention-for-clusters.md
[kops]: https://github.com/kubernetes/kops
[helm]: https://helm.sh
[AWS CLI]: https://aws.amazon.com/cli/
[ruby]: https://www.ruby-lang.org/
[docker]: https://www.docker.com/
[PagerDuty]: https://www.pagerduty.com/
